## Awesome libraries
[Composer](https://github.com/mosaicml/composer):
A library that applies optimizations to speed up training process of large neural nets

## Awesome articles
[Making Deep Learning go Brrrr](https://horace.io/brrr_intro.html):
An article that introduces first principles of performance optimization in deep learning

[How Nvidiaâ€™s CUDA Monopoly In Machine Learning Is Breaking - OpenAI Triton And PyTorch 2.0](https://www.semianalysis.com/p/nvidiaopenaitritonpytorch)

## Awesome papers
### Knowledge graphs
[Crawling the internal knowledge graphs of large language models](https://arxiv.org/abs/2301.12810) 

### Quantization
[The case for 4-bit precision: k-bit Inference Scaling Laws](https://arxiv.org/abs/2212.09720)
[GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
